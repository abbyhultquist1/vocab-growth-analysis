{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load CDI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680 word columns, 122 children\n"
     ]
    }
   ],
   "source": [
    "CDI = pd.read_csv('/Users/abbyhultquist/Documents/First Year Project/long_categorization_6.csv')\n",
    "CDI['child_id'] = CDI['child_id'].astype(str)\n",
    "\n",
    "metadata_cols = CDI.columns[:21].tolist()\n",
    "word_cols     = CDI.columns[21:].tolist()\n",
    "\n",
    "print(f\"{len(word_cols)} word columns, {CDI['child_id'].nunique()} children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load similarity matrix and build known-word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  child_id  session_num                                        known_words  \\\n",
      "0     4139            1  [bear, bird, cat, dog, duck, frog, pig, sheep,...   \n",
      "1     4139            2  [bear, bird, cat, dog, duck, frog, owl, pig, s...   \n",
      "2     4139            3  [bear, bird, cat, cow, dog, duck, frog, owl, p...   \n",
      "3     4139            4  [bear, bird, butterfly, cat, cow, dog, duck, e...   \n",
      "4     4139            5  [bear, bee, bird, bunny, butterfly, cat, cow, ...   \n",
      "\n",
      "   num_known  \n",
      "0         60  \n",
      "1         89  \n",
      "2        112  \n",
      "3        122  \n",
      "4        149  \n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = (\n",
    "    pd.read_csv(\"/Users/abbyhultquist/Documents/First Year Project/similarity_mat/nouns.csv\")\n",
    "    .set_index(\"Unnamed: 0\")\n",
    ")\n",
    "sim_vocab = set(similarity_matrix.index) & set(similarity_matrix.columns)\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# For each child x session, record which sim_vocab words they know\n",
    "known_words_df = pd.DataFrame([\n",
    "    {\n",
    "        \"child_id\":   str(row[\"child_id\"]),\n",
    "        \"session_num\": row[\"session_num\"],\n",
    "        \"known_words\": [w for w in word_cols if row[w] == 1 and w in sim_vocab],\n",
    "    }\n",
    "    for _, row in CDI.iterrows()\n",
    "])\n",
    "known_words_df[\"num_known\"] = known_words_df[\"known_words\"].str.len()\n",
    "\n",
    "talker_lookup = CDI[[\"child_id\", \"Talker_Type\"]].drop_duplicates().assign(child_id=lambda d: d[\"child_id\"].astype(str))\n",
    "\n",
    "print(known_words_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build observed semantic graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 1162 graphs (122 children × multiple sessions)\n"
     ]
    }
   ],
   "source": [
    "def build_semantic_graph(words, sim_matrix, threshold):\n",
    "    \"\"\"Connect words whose cosine similarity exceeds threshold.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(words)\n",
    "    for i, wi in enumerate(words):\n",
    "        for wj in words[i+1:]:\n",
    "            sim = sim_matrix.loc[wi, wj]\n",
    "            if sim >= threshold:\n",
    "                G.add_edge(wi, wj, weight=float(sim))\n",
    "    return G\n",
    "\n",
    "# Build graphs for each child x session\n",
    "graphs = {}\n",
    "for _, row in known_words_df.iterrows():\n",
    "    child_id = row[\"child_id\"]\n",
    "    session = row[\"session_num\"]\n",
    "    words = row[\"known_words\"]\n",
    "    \n",
    "    if len(words) > 0:\n",
    "        key = (child_id, session)\n",
    "        graphs[key] = build_semantic_graph(words, similarity_matrix, THRESHOLD)\n",
    "\n",
    "print(f\"Built {len(graphs)} graphs ({known_words_df['child_id'].nunique()} children × multiple sessions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62aadd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graphs by Session ===\n",
      "    session_num  num_graphs  num_children\n",
      "0             1         118           118\n",
      "1             2         110           110\n",
      "2             3         111           111\n",
      "3             4         108           108\n",
      "4             5         105           105\n",
      "5             6          93            93\n",
      "6             7          89            89\n",
      "7             8          93            93\n",
      "8             9          84            84\n",
      "9            10          83            83\n",
      "10           11          82            82\n",
      "11           12          85            85\n",
      "12           13           1             1\n",
      "\n",
      "Total graphs built: 1162\n",
      "Total children: 121\n"
     ]
    }
   ],
   "source": [
    "# Summary: graphs per session and children per session\n",
    "session_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"session_num\": session,\n",
    "        \"num_graphs\": sum(1 for (cid, s) in graphs.keys() if s == session),\n",
    "        \"num_children\": len(set(cid for (cid, s) in graphs.keys() if s == session))\n",
    "    }\n",
    "    for session in sorted(known_words_df[\"session_num\"].unique())\n",
    "])\n",
    "\n",
    "print(\"=== Graphs by Session ===\")\n",
    "print(session_summary)\n",
    "print(f\"\\nTotal graphs built: {len(graphs)}\")\n",
    "print(f\"Total children: {len(set(cid for (cid, s) in graphs.keys()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute graph metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed metrics for 1162 graphs\n",
      "  child_id  session_num  num_nodes  avg_degree  avg_clustering  \\\n",
      "0     4139            1         60   21.700000        0.761027   \n",
      "1     4139            2         89   32.898876        0.779075   \n",
      "2     4139            3        112   39.267857        0.780101   \n",
      "3     4139            4        122   42.950820        0.779165   \n",
      "4     4139            5        149   53.302013        0.782518   \n",
      "\n",
      "   avg_geodesic_distance Talker_Type  \n",
      "0               2.395091         NaN  \n",
      "1               2.232635         NaN  \n",
      "2               2.272683         NaN  \n",
      "3               2.232760         NaN  \n",
      "4               2.210230         NaN  \n",
      "\n",
      "Metrics by Talker_Type:\n",
      "             session_num  num_nodes  avg_degree  avg_clustering  \\\n",
      "Talker_Type                                                       \n",
      "Faller             6.318     95.545      30.154           0.760   \n",
      "LB                 6.409    117.695      40.109           0.702   \n",
      "PLT                6.575     47.938      14.780           0.605   \n",
      "TT                 6.374    201.829      69.340           0.769   \n",
      "\n",
      "             avg_geodesic_distance  \n",
      "Talker_Type                         \n",
      "Faller                       2.387  \n",
      "LB                           1.902  \n",
      "PLT                          1.704  \n",
      "TT                           2.119  \n"
     ]
    }
   ],
   "source": [
    "def graph_metrics(child_id, session, G):\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    avg_degree     = 2 * m / n if n > 0 else 0\n",
    "    avg_clustering = nx.average_clustering(G) if n > 0 else 0\n",
    "\n",
    "    # Geodesic on largest connected component only if it has >1 node\n",
    "    avg_geodesic = 0\n",
    "    if m > 0:\n",
    "        lcc = G.subgraph(max(nx.connected_components(G), key=len)).copy()\n",
    "        if lcc.number_of_nodes() > 1:\n",
    "            avg_geodesic = nx.average_shortest_path_length(lcc)\n",
    "\n",
    "    return {\n",
    "        \"child_id\":              child_id,\n",
    "        \"session_num\":           session,\n",
    "        \"num_nodes\":             n,\n",
    "        \"avg_degree\":            avg_degree,\n",
    "        \"avg_clustering\":        avg_clustering,\n",
    "        \"avg_geodesic_distance\": avg_geodesic,\n",
    "    }\n",
    "\n",
    "obs_metrics = pd.DataFrame([graph_metrics(cid, sess, G) for (cid, sess), G in graphs.items()])\n",
    "obs_metrics_full = obs_metrics.merge(talker_lookup, on=\"child_id\", how=\"left\")\n",
    "\n",
    "print(f\"Computed metrics for {len(obs_metrics)} graphs\")\n",
    "print(obs_metrics_full.head())\n",
    "print(\"\\nMetrics by Talker_Type:\")\n",
    "print(obs_metrics_full.groupby(\"Talker_Type\").mean(numeric_only=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5a: Erdős–Rényi random graphs (preserve n and m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1   # more samples = more stable baseline (was 10)\n",
    "\n",
    "def er_random_metrics(child_id, session, G, n_samples=N_SAMPLES):\n",
    "    \"\"\"Generate ER random graphs with same n and m; average metrics.\"\"\"\n",
    "    n, m = G.number_of_nodes(), G.number_of_edges()\n",
    "    rows = []\n",
    "    for _ in range(n_samples):\n",
    "        R = nx.gnm_random_graph(n, m)\n",
    "        rows.append(graph_metrics(child_id, session, R))\n",
    "    result_dict = pd.DataFrame(rows).mean(numeric_only=True).to_dict()\n",
    "    result_dict.update({\"child_id\": child_id, \"session_num\": session})\n",
    "    return result_dict\n",
    "\n",
    "er_metrics = pd.DataFrame([er_random_metrics(cid, sess, G) for (cid, sess), G in graphs.items()])\n",
    "er_metrics = er_metrics.rename(columns={c: f\"er_{c}\" for c in er_metrics.columns if c not in [\"child_id\", \"session_num\"]})\n",
    "er_metrics[\"child_id\"] = er_metrics[\"child_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5b: Semantic random graphs (preserve n, random words from sim_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_random_metrics(child_id, session, G, sim_matrix, vocab, threshold, n_samples=N_SAMPLES):\n",
    "    \"\"\"Sample n random words from full vocab, connect by similarity; average metrics.\"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    vocab_list = list(vocab)\n",
    "    rows = []\n",
    "    for _ in range(n_samples):\n",
    "        words = random.sample(vocab_list, min(n, len(vocab_list)))\n",
    "        R = build_semantic_graph(words, sim_matrix, threshold)\n",
    "        rows.append(graph_metrics(child_id, session, R))\n",
    "    result_dict = pd.DataFrame(rows).mean(numeric_only=True).to_dict()\n",
    "    result_dict.update({\"child_id\": child_id, \"session_num\": session})\n",
    "    return result_dict\n",
    "\n",
    "sem_metrics = pd.DataFrame([\n",
    "    semantic_random_metrics(cid, sess, G, similarity_matrix, sim_vocab, THRESHOLD)\n",
    "    for (cid, sess), G in graphs.items()\n",
    "])\n",
    "sem_metrics = sem_metrics.rename(columns={c: f\"sem_{c}\" for c in sem_metrics.columns if c not in [\"child_id\", \"session_num\"]})\n",
    "sem_metrics[\"child_id\"] = sem_metrics[\"child_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Merge and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison shape: (1162, 19)\n",
      "               n  obs_num_nodes  er_num_nodes  sem_num_nodes  obs_degree  \\\n",
      "Talker_Type                                                                \n",
      "Faller        22        95.5455       95.5455        95.5455     30.1544   \n",
      "LB           279       117.6953      117.6953       117.6953     40.1088   \n",
      "PLT           80        47.9375       47.9375        47.9375     14.7805   \n",
      "TT           626       201.8291      201.8291       201.8291     69.3403   \n",
      "\n",
      "             er_degree  sem_degree  obs_clustering  er_clustering  \\\n",
      "Talker_Type                                                         \n",
      "Faller         30.1544     33.5890          0.7597         0.3125   \n",
      "LB             40.1088     41.6634          0.7019         0.3077   \n",
      "PLT            14.7805     16.7822          0.6048         0.2515   \n",
      "TT             69.3403     71.4373          0.7686         0.3371   \n",
      "\n",
      "             sem_clustering  obs_geodesic  er_geodesic  sem_geodesic  \n",
      "Talker_Type                                                           \n",
      "Faller               0.7604        2.3871       1.6956        1.7719  \n",
      "LB                   0.6966        1.9015       1.6861        1.7975  \n",
      "PLT                  0.5842        1.7041       1.6375        1.4562  \n",
      "TT                   0.7709        2.1194       1.6724        2.0334  \n"
     ]
    }
   ],
   "source": [
    "comparison = (\n",
    "    obs_metrics_full\n",
    "    .merge(er_metrics,  on=[\"child_id\", \"session_num\"], how=\"left\")\n",
    "    .merge(sem_metrics, on=[\"child_id\", \"session_num\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Observed minus each baseline\n",
    "for metric in [\"avg_clustering\", \"avg_geodesic_distance\"]:\n",
    "    comparison[f\"{metric}_vs_er\"]  = comparison[metric] - comparison[f\"er_{metric}\"]\n",
    "    comparison[f\"{metric}_vs_sem\"] = comparison[metric] - comparison[f\"sem_{metric}\"]\n",
    "\n",
    "by_talker = (\n",
    "    comparison.groupby(\"Talker_Type\")\n",
    "    .agg(\n",
    "        n=(\"child_id\", \"count\"),\n",
    "        # Nodes\n",
    "        obs_num_nodes=(\"num_nodes\", \"mean\"),\n",
    "        er_num_nodes=(\"er_num_nodes\", \"mean\"),\n",
    "        sem_num_nodes=(\"sem_num_nodes\", \"mean\"),\n",
    "        # Degree\n",
    "        obs_degree=(\"avg_degree\", \"mean\"),\n",
    "        er_degree=(\"er_avg_degree\", \"mean\"),\n",
    "        sem_degree=(\"sem_avg_degree\", \"mean\"),\n",
    "        # Clustering\n",
    "        obs_clustering=(\"avg_clustering\", \"mean\"),\n",
    "        er_clustering=(\"er_avg_clustering\", \"mean\"),\n",
    "        sem_clustering=(\"sem_avg_clustering\", \"mean\"),\n",
    "        # Geodesic\n",
    "        obs_geodesic=(\"avg_geodesic_distance\", \"mean\"),\n",
    "        er_geodesic=(\"er_avg_geodesic_distance\", \"mean\"),\n",
    "        sem_geodesic=(\"sem_avg_geodesic_distance\", \"mean\"),\n",
    "        \n",
    "    )\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(f\"Comparison shape: {comparison.shape}\")\n",
    "print(by_talker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7050198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Observed minus baseline (positive = observed higher) ===\n",
      "               n  nodes_vs_er  degree_vs_er  clustering_vs_er  geodesic_vs_er  \\\n",
      "Talker_Type                                                                     \n",
      "Faller        22          0.0           0.0            0.4472          0.6915   \n",
      "LB           279          0.0           0.0            0.3942          0.2154   \n",
      "PLT           80          0.0           0.0            0.3533          0.0666   \n",
      "TT           626          0.0           0.0            0.4315          0.4470   \n",
      "\n",
      "             nodes_vs_sem  degree_vs_sem  clustering_vs_sem  geodesic_vs_sem  \n",
      "Talker_Type                                                                   \n",
      "Faller                0.0        -3.4346            -0.0007           0.6152  \n",
      "LB                    0.0        -1.5546             0.0053           0.1040  \n",
      "PLT                   0.0        -2.0017             0.0206           0.2479  \n",
      "TT                    0.0        -2.0970            -0.0023           0.0860  \n"
     ]
    }
   ],
   "source": [
    "diff_by_talker = pd.DataFrame({\n",
    "    \"n\": by_talker[\"n\"],\n",
    "    # vs ER\n",
    "    \"nodes_vs_er\":      by_talker[\"obs_num_nodes\"]   - by_talker[\"er_num_nodes\"],\n",
    "    \"degree_vs_er\":     by_talker[\"obs_degree\"]       - by_talker[\"er_degree\"],\n",
    "    \"clustering_vs_er\": by_talker[\"obs_clustering\"]   - by_talker[\"er_clustering\"],\n",
    "    \"geodesic_vs_er\":   by_talker[\"obs_geodesic\"]     - by_talker[\"er_geodesic\"],\n",
    "    # vs Semantic Random\n",
    "    \"nodes_vs_sem\":      by_talker[\"obs_num_nodes\"]  - by_talker[\"sem_num_nodes\"],\n",
    "    \"degree_vs_sem\":     by_talker[\"obs_degree\"]      - by_talker[\"sem_degree\"],\n",
    "    \"clustering_vs_sem\": by_talker[\"obs_clustering\"]  - by_talker[\"sem_clustering\"],\n",
    "    \"geodesic_vs_sem\":   by_talker[\"obs_geodesic\"]    - by_talker[\"sem_geodesic\"],\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== Observed minus baseline (positive = observed higher) ===\")\n",
    "print(diff_by_talker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved graph_metrics_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "comparison.to_csv(\"graph_metrics_comparison.csv\", index=False)\n",
    "print(\"Saved graph_metrics_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
